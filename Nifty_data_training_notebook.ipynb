{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "mpl.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, f1_score, precision_score, \\\n",
    "                            recall_score, accuracy_score, recall_score, \\\n",
    "                            roc_curve, auc, roc_auc_score, precision_recall_curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import config\n",
    "\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import operator\n",
    "import time\n",
    "from pprint import pprint as pp\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = None\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data into memory\n",
    "df = pd.read_csv('nifty_train_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture 30 return data into y \n",
    "y = df['Returns_30day']\n",
    "Counter(y)\n",
    "\n",
    "# Drop 30 day return column\n",
    "# We know this data already based on around 30 day return\n",
    "# We ll use this data to predict 30 day return.\n",
    "df.drop(['Returns_30day'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture dataframe values in x\n",
    "X = df.values\n",
    "# table dimensions\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table dimensions\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data on 80% and test of rest 20%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#from tpot.builtins import StackingEstimator\n",
    "\n",
    "# Create big array\n",
    "x1 = X[:80000]\n",
    "y1 = y[:80000]\n",
    "# Creating training and testing data\n",
    "training_features, testing_features, training_target, testing_target = train_test_split(x1, y1, \n",
    "                                                                                        train_size=0.80, \n",
    "                                                                                        test_size=0.20,\n",
    "                                                                                        stratify=y1,\n",
    "                                                                                        random_state=42)\n",
    "# Over-sampling training data\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(training_features, training_target)\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_resampled = normalizer.fit_transform(X_resampled)\n",
    "testing_features = normalizer.transform(testing_features)\n",
    "\n",
    "# Training Model\n",
    "exported_pipeline = RandomForestClassifier(bootstrap=False, max_features='auto', \n",
    "                                           min_samples_leaf=1, min_samples_split=5, \n",
    "                                           n_estimators=80, \n",
    "                                           criterion='gini',\n",
    "                                           n_jobs=-1)\n",
    "\n",
    "exported_pipeline.fit(X_resampled, y_resampled)\n",
    "y_pred = exported_pipeline.predict(testing_features)\n",
    "\n",
    "print(time.ctime())\n",
    "\n",
    "print('Training test accuracy:', accuracy_score(testing_target, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate data test\n",
    "validation_testing_features = normalizer.transform(X[:80000])\n",
    "validation_y_actual = y[:80000]\n",
    "validation_y_pred = exported_pipeline.predict(validation_testing_features)\n",
    "print('Validation test accuracy:', accuracy_score(validation_y_actual, validation_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print pipeline used in RandomForest\n",
    "exported_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print indicator importance value\n",
    "\n",
    "def full_form(x):\n",
    "    try:\n",
    "        return config.tickers[x]\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "importance = exported_pipeline.feature_importances_\n",
    "feature_set = {\n",
    "    'Feature_name': df.columns,\n",
    "    'Importance': importance\n",
    "}\n",
    "\n",
    "f = pd.DataFrame(feature_set).sort_values(['Importance'], ascending=False)\n",
    "f['full_name'] = f.Feature_name.apply(full_form)\n",
    "f['Importance'] = f['Importance'].apply(lambda x: np.round(x,4)) \n",
    "bad_features = f[f.Importance < 0.01].Feature_name\n",
    "print('Low importance features:', bad_features.values)\n",
    "f.to_csv('feature_importance.csv')\n",
    "f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion plot\n",
    "\n",
    "def confusion_plot(y_test, y_predicted, classes):\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "    # Scaling values\n",
    "    cm = cm.astype('float')*100 / cm.sum(axis=1)[:, np.newaxis] \n",
    "    np.set_printoptions(suppress=True)\n",
    "    mpl.rc(\"figure\", figsize=(7,7)) #subplot size\n",
    "\n",
    "    hm = sns.heatmap(cm, \n",
    "                cbar=False,\n",
    "                annot=True, \n",
    "                square=True,\n",
    "                yticklabels=classes,\n",
    "                xticklabels=classes,\n",
    "                cmap='Blues',\n",
    "                linewidths =.5,\n",
    "                annot_kws={'size':14} #text size\n",
    "                )\n",
    "    plt.title('Prediction matrix')\n",
    "    plt.ylabel('Actual class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "confusion_plot(validation_y_actual, validation_y_pred,[-5,-4,-3,-2,-1,0,1,2,3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate predicated and probability values\n",
    "predicted = np.argmax(exported_pipeline.predict_proba(validation_testing_features), axis=1) - 5\n",
    "prob = np.max(exported_pipeline.predict_proba(validation_testing_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how predication faired with actual returns\n",
    "cutoff_prob = 0.4\n",
    "cutoff_returns = 2\n",
    "\n",
    "def _color_red_or_green(val):\n",
    "    color = 'red' if val < 0.5 else 'green'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "    'predicted': validation_y_pred,\n",
    "    'actual_returns': validation_y_actual,\n",
    "    'prob': prob\n",
    "})\n",
    "\n",
    "\n",
    "df_result_final = df_result[(np.absolute(df_result.predicted)>=cutoff_returns) \n",
    "                            & (df_result.prob>=cutoff_prob) \n",
    "#                             &(df_result.predicted==df_result.true_label)\n",
    "                           ]\n",
    "print(len(df_result_final)/len(df_result))\n",
    "df_result_final.style.applymap(_color_red_or_green, subset=['prob'])\n",
    "#df_result_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets trade using this data\n",
    "trade_amount = 1000\n",
    "#transaction_charge = 0.25/100*2\n",
    "#crypto_slippage = 0.97**2\n",
    "#currency_slippage = 0.995**2\n",
    "\n",
    "def returns(pred, true_):\n",
    "    if pred>=2: \n",
    "        if true_>=pred:\n",
    "            return pred\n",
    "        else:\n",
    "            return true_\n",
    "    if pred<=-2:\n",
    "        if true_<=pred:\n",
    "            return -pred\n",
    "        else:\n",
    "            return -true_\n",
    "\n",
    "df_result_final['Action'] = df_result_final['predicted'].apply(lambda x: 'Long' if x>0 else 'Short')\n",
    "df_result_final['realised_returns'] = df_result_final.apply(lambda row: returns(row['predicted'], row['actual_returns']), axis=1)\n",
    "df_result_final['profit'] = df_result_final.apply(lambda row: row['prob']*row['realised_returns']*np.sqrt(np.absolute(row['predicted'])/2)*0.01*trade_amount, axis=1)\n",
    "print('Net profit:', df_result_final['profit'].sum())\n",
    "# Display top 10 trades\n",
    "df_result_final.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot actual and predicated returns on a chart\n",
    "confusion_plot(df_result_final.actual_returns, df_result_final.predicted, [-5,-4,-3,-2,-1,0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What went wrong\n",
    "print('Wrong predictions')\n",
    "df_result_final[df_result_final.actual_returns!=df_result_final.predicted].head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we lose\n",
    "print('Lost money')\n",
    "df_result_final[df_result_final.realised_returns<0].head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much did we make\n",
    "print('Net profit:', df_result_final['profit'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long did we trade\n",
    "print('Number of days', validation_y_pred.shape[0]/96/4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
